{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "custom_vs_builtin_batchnorm.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDjTprK0EzeY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnyv5jWqF3aA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "91871904-73c0-4064-feea-3e5e695636f5"
      },
      "source": [
        "transform = transforms.Compose(\n",
        "    [ transforms.ToTensor(),\n",
        "      transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n",
        "    ])\n",
        "\n",
        "batch_size = 4\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, \n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=True,\n",
        "                                          num_workers=2\n",
        "                                         )\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, \n",
        "                                         batch_size=batch_size,\n",
        "                                         shuffle=False, \n",
        "                                         num_workers=2\n",
        "                                        )\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "170500096it [00:06, 27123227.82it/s]                               \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_Hmock_Go3p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "outputId": "634bfb3b-9b23-4c67-a960-e129a82ef41f"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# functions to show an image\n",
        "\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5     # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "# get some random training images\n",
        "dataiter = iter(trainloader)\n",
        "images, labels = dataiter.next()\n",
        "\n",
        "# show images\n",
        "imshow(torchvision.utils.make_grid(images))\n",
        "# print labels\n",
        "print(' '.join('%5s' % classes[labels[j]] for j in range(4)))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAB6CAYAAACvHqiXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "  car   cat   dog truck\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DkePeNLQLf0N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# input = torch.randn(2,3,2,2) * 4 + 3\n",
        "# mean = torch.mean(input, dim=(0,2,3))\n",
        "# print(mean[None, :, None, None])\n",
        "# input = input - mean[None, :, None, None]\n",
        "# print(input)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQCVwokd4s1M",
        "colab_type": "text"
      },
      "source": [
        "# **Batch normalization algorithm**\n",
        "\n",
        "**Input**: Values of $x$ over a mini-batch: $\\beta = \\left \\{ x_{1..m} \\right \\};$ \n",
        "\n",
        "Parameters to be learned: $\\gamma, \\beta$\n",
        "\n",
        "**Output**:  $ \\left \\{ y_{i} = \\mathrm{BN_{\\gamma,\\beta}}(x_i)  \\right \\} $ \n",
        "\n",
        "$\\mu_{B} \\leftarrow  \\frac{1}{m}\\sum_{i=1}^{m}x_{i}$    //mini batch mean\n",
        " \n",
        " $\\sigma_{B}^{2} \\leftarrow \\frac{1}{m}\\sum_{i=1}^{m} (x_{i} - \\mu_{B})^{2}$    //mini-batch variance \n",
        " \n",
        " $\\widehat{x}_{i} \\leftarrow \\frac{x_{i} - \\mu_{B}}{\\sqrt{\\sigma_{B}^{2} + \\epsilon}}$    //normalize \n",
        " \n",
        " $y_{i} \\leftarrow \\gamma \\widehat{x}_{i} + \\beta \\equiv \\mathrm{BN_{\\gamma,\\beta}}(x_i)$     //scale and shift \n",
        " \n",
        " Now at training time we calculate mean and variance for each mini-batch. But in inference time we do not calculate mean and variance for test data. Rather we calculate population average of mean and variances after training, using all the mini-batch mean and variances. Then at evaluation, we fix the mean and variance to this value. \n",
        " \n",
        " $\\mathbb{E}[x] \\leftarrow \\mathbb{E}_{B}[\\mu_{B}]$\n",
        " \n",
        " $Var[x] \\leftarrow \\frac{m}{m-1} \\mathbb{E}_{B}[\\sigma_{B}^{2}]$\n",
        " \n",
        " But it is difficult to keep track of all the mini-batch mean and variances. So, that's why, exponential weighted *moving average* is used to update population mean and variance\n",
        " \n",
        " $\\mu_{mov} = \\alpha * \\mu_{mov} + (1-\\alpha)*\\mu_{B}$\n",
        " \n",
        " $\\sigma^{2}_{mov} = \\alpha * \\sigma_{mov}^{2} + (1-\\alpha)*\\sigma_{B}^{2}$\n",
        " \n",
        "\n",
        "\n",
        "> Done in Line $22$ and $24$\n",
        "\n",
        "\n",
        " \n",
        " Here $\\alpha$ is the momentum and the ones with $B$ subscript are mini-batch mean and variance.\n",
        " \n",
        " Also a note that the *moving mean and variance* are calculated at training time but not at evaluation. That's why the whole calculation is in \"if\" block of \"self.training\" at line $12$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Eww9YaSHXO7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Batch Normalization \n",
        "class MyBatchNorm2d(nn.BatchNorm2d):\n",
        "  def __init__(self, num_features, eps = 1e-5, momentum=0.1, affine=True, track_running_stats=True):\n",
        "    super(MyBatchNorm2d, self).__init__(num_features, eps = 1e-5, momentum=0.1, affine=True, track_running_stats=True)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    self._check_input_dim(x) \n",
        "    \n",
        "    momentum = self.momentum\n",
        "    \n",
        "#     for training\n",
        "    if self.training:\n",
        "    # (batch_size, no_of_channel, row, column). We want to take mean and variance over 3 channels. The normalization is done along channel axis.\n",
        "#     Why? Hint: Weight Sharing\n",
        "      mean = torch.mean(x, dim=(0, 2, 3))\n",
        "      var = torch.var(x, dim=(0, 2, 3), unbiased=False) # biased variance is used in training.\n",
        "      m = x.numel() / x.size(1)  # no of elements(batch_size * channel_no * row * column) / channel no = no of elements of each channel per batch\n",
        "      \n",
        "#     According to batchnorm paper, Take population average for running mean and variance\n",
        "#     and take an exponential weighted moving average of those two.\n",
        "      with torch.no_grad():\n",
        "        self.running_mean = momentum * mean + (1 - momentum) * self.running_mean\n",
        "        \n",
        "        self.running_var = momentum * var * m / (m-1) + (1 - momentum) * self.running_var\n",
        "\n",
        "    #   for evaluation\n",
        "    else:\n",
        "      mean = self.running_mean # for evaluation use the running_mean that was used in training\n",
        "      var = self.running_var # same\n",
        "      \n",
        "#     Normalization eqn, x = (x - mu) / sqrt(var + epsilon)  \n",
        "    x = ( x - mean[None, :, None, None]) / (torch.sqrt(var[None, :, None, None] + self.eps))  \n",
        "    \n",
        "#     affine is used to update the parameter (gamma, beta). So weight = gamma and bias = beta.\n",
        "#   The equation of BatchNorm for output, y = gamma * x + Beta \n",
        "    if self.affine:\n",
        "      x = x * self.weight[None,:,None,None] + self.bias[None,:,None,None]\n",
        "    \n",
        "    return x "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91QsZJhTPFIE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "94619367-3d3d-4376-cb11-e2733b8b8002"
      },
      "source": [
        "# check if BatchNorm works compared with BatchNorm of pytorch.\n",
        "x = torch.randn(2,3,4,4) * 8 + 3\n",
        "my_bn = MyBatchNorm2d(3, affine=True)\n",
        "torch_bn = nn.BatchNorm2d(3, affine=True)\n",
        "out1 = my_bn(x)\n",
        "out2 = torch_bn(x)\n",
        "print('Max diff: ', (out1 - out2).abs().max().item())"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max diff:  0.7433098554611206\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AyPLSYpoG74q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "outputId": "2b92c9d3-7d49-457f-b8e1-143a59e5859e"
      },
      "source": [
        "class ModelWithMyBatchNorm(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ModelWithMyBatchNorm, self).__init__()\n",
        "        \n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.bn1 = MyBatchNorm2d(6)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.bn2 = MyBatchNorm2d(16)\n",
        "        \n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.bn1((self.conv1(x)))))\n",
        "        x = self.pool(F.relu(self.bn2((self.conv2(x)))))\n",
        "        \n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "model = ModelWithMyBatchNorm()\n",
        "print(model)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ModelWithMyBatchNorm(\n",
            "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (bn1): MyBatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (bn2): MyBatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
            "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
            "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFyX-WSXWGFg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "outputId": "b728534d-3cbf-4924-f693-82a46e8206f7"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model.to(device)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ModelWithMyBatchNorm(\n",
              "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
              "  (bn1): MyBatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
              "  (bn2): MyBatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
              "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
              "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5u4jgkVTKGr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3rY-8fjUq7I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 586
        },
        "outputId": "c4894bc9-eeae-4c86-f584-af9642d362ee"
      },
      "source": [
        "EPOCHS = 5\n",
        "print_every = 2000\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  \n",
        "  running_loss = 0.0\n",
        "  \n",
        "  for iters, data in enumerate(trainloader, 0):\n",
        "    inputs, labels = data[0].to(device), data[1].to(device) \n",
        "    \n",
        "#     make gradients parameter zero\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "#     forward propagation\n",
        "    outputs = model(inputs)\n",
        "#   calculate loss\n",
        "    loss = criterion(outputs, labels)\n",
        "#   do backward propagation\n",
        "    loss.backward()\n",
        "#   update weights\n",
        "    optimizer.step()\n",
        "    \n",
        "#     print result\n",
        "    running_loss += loss.item()\n",
        "    if iters % print_every == 1999:\n",
        "      print(\"epoch: %d, iterations: %5d, loss: %.3f\" % \n",
        "           (epoch + 1, iters + 1, running_loss / print_every))\n",
        "      \n",
        "      running_loss = 0.0\n",
        "      \n",
        "print(\"Finished Training\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 1, iterations:  2000, loss: 1.875\n",
            "epoch: 1, iterations:  4000, loss: 1.666\n",
            "epoch: 1, iterations:  6000, loss: 1.587\n",
            "epoch: 1, iterations:  8000, loss: 1.526\n",
            "epoch: 1, iterations: 10000, loss: 1.477\n",
            "epoch: 1, iterations: 12000, loss: 1.432\n",
            "epoch: 2, iterations:  2000, loss: 1.381\n",
            "epoch: 2, iterations:  4000, loss: 1.349\n",
            "epoch: 2, iterations:  6000, loss: 1.320\n",
            "epoch: 2, iterations:  8000, loss: 1.302\n",
            "epoch: 2, iterations: 10000, loss: 1.320\n",
            "epoch: 2, iterations: 12000, loss: 1.274\n",
            "epoch: 3, iterations:  2000, loss: 1.201\n",
            "epoch: 3, iterations:  4000, loss: 1.227\n",
            "epoch: 3, iterations:  6000, loss: 1.220\n",
            "epoch: 3, iterations:  8000, loss: 1.192\n",
            "epoch: 3, iterations: 10000, loss: 1.188\n",
            "epoch: 3, iterations: 12000, loss: 1.194\n",
            "epoch: 4, iterations:  2000, loss: 1.152\n",
            "epoch: 4, iterations:  4000, loss: 1.132\n",
            "epoch: 4, iterations:  6000, loss: 1.133\n",
            "epoch: 4, iterations:  8000, loss: 1.106\n",
            "epoch: 4, iterations: 10000, loss: 1.139\n",
            "epoch: 4, iterations: 12000, loss: 1.106\n",
            "epoch: 5, iterations:  2000, loss: 1.062\n",
            "epoch: 5, iterations:  4000, loss: 1.082\n",
            "epoch: 5, iterations:  6000, loss: 1.084\n",
            "epoch: 5, iterations:  8000, loss: 1.080\n",
            "epoch: 5, iterations: 10000, loss: 1.089\n",
            "epoch: 5, iterations: 12000, loss: 1.074\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkWQS8Ie3Ogn",
        "colab_type": "text"
      },
      "source": [
        "Let's implement without batch normalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmbopYIg1yr1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "outputId": "e438a203-2827-4bc4-b76e-6a083f0e9e66"
      },
      "source": [
        "class ModelWithBatchNorm(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ModelWithBatchNorm, self).__init__()\n",
        "        \n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.bn1 = nn.BatchNorm2d(6)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.bn2 = nn.BatchNorm2d(16)\n",
        "        \n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.bn1((self.conv1(x)))))\n",
        "        x = self.pool(F.relu(self.bn2((self.conv2(x)))))\n",
        "        \n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "model2 = ModelWithBatchNorm()\n",
        "print(model2)\n",
        "model2.to(device)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ModelWithBatchNorm(\n",
            "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (bn1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
            "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
            "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ModelWithBatchNorm(\n",
              "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
              "  (bn1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
              "  (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
              "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
              "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66V4_n_j2DPG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer2 = optim.Adam(model2.parameters(), lr=0.001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSV54M4m2Min",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 586
        },
        "outputId": "8aa15e9f-a378-4300-8437-652fd7f46591"
      },
      "source": [
        "for epoch in range(EPOCHS):\n",
        "  \n",
        "  running_loss = 0.0\n",
        "  \n",
        "  for iters, data in enumerate(trainloader, 0):\n",
        "    inputs, labels = data[0].to(device), data[1].to(device) \n",
        "    \n",
        "#     make gradients parameter zero\n",
        "    optimizer2.zero_grad()\n",
        "    \n",
        "#     forward propagation\n",
        "    outputs = model2(inputs)\n",
        "#   calculate loss\n",
        "    loss = criterion(outputs, labels)\n",
        "#   do backward propagation\n",
        "    loss.backward()\n",
        "#   update weights\n",
        "    optimizer2.step()\n",
        "    \n",
        "#     print result\n",
        "    running_loss += loss.item()\n",
        "    if iters % print_every == 1999:\n",
        "      print(\"epoch: %d, iterations: %5d, loss: %.3f\" % \n",
        "           (epoch + 1, iters + 1, running_loss / print_every))\n",
        "      \n",
        "      running_loss = 0.0\n",
        "      \n",
        "print(\"Finished Training\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 1, iterations:  2000, loss: 1.891\n",
            "epoch: 1, iterations:  4000, loss: 1.660\n",
            "epoch: 1, iterations:  6000, loss: 1.554\n",
            "epoch: 1, iterations:  8000, loss: 1.509\n",
            "epoch: 1, iterations: 10000, loss: 1.452\n",
            "epoch: 1, iterations: 12000, loss: 1.428\n",
            "epoch: 2, iterations:  2000, loss: 1.359\n",
            "epoch: 2, iterations:  4000, loss: 1.357\n",
            "epoch: 2, iterations:  6000, loss: 1.318\n",
            "epoch: 2, iterations:  8000, loss: 1.330\n",
            "epoch: 2, iterations: 10000, loss: 1.293\n",
            "epoch: 2, iterations: 12000, loss: 1.296\n",
            "epoch: 3, iterations:  2000, loss: 1.219\n",
            "epoch: 3, iterations:  4000, loss: 1.243\n",
            "epoch: 3, iterations:  6000, loss: 1.231\n",
            "epoch: 3, iterations:  8000, loss: 1.205\n",
            "epoch: 3, iterations: 10000, loss: 1.212\n",
            "epoch: 3, iterations: 12000, loss: 1.222\n",
            "epoch: 4, iterations:  2000, loss: 1.167\n",
            "epoch: 4, iterations:  4000, loss: 1.184\n",
            "epoch: 4, iterations:  6000, loss: 1.151\n",
            "epoch: 4, iterations:  8000, loss: 1.150\n",
            "epoch: 4, iterations: 10000, loss: 1.149\n",
            "epoch: 4, iterations: 12000, loss: 1.133\n",
            "epoch: 5, iterations:  2000, loss: 1.099\n",
            "epoch: 5, iterations:  4000, loss: 1.127\n",
            "epoch: 5, iterations:  6000, loss: 1.099\n",
            "epoch: 5, iterations:  8000, loss: 1.115\n",
            "epoch: 5, iterations: 10000, loss: 1.104\n",
            "epoch: 5, iterations: 12000, loss: 1.119\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4T_ep3O22FI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "a98c9096-7d75-483a-9202-278c047308f0"
      },
      "source": [
        "correct1, correct2 = 0, 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "  for data in testloader:\n",
        "    images, labels = data[0].to(device), data[1].to(device)\n",
        "    output1 = model(images)\n",
        "    output2 = model2(images)\n",
        "    \n",
        "    _, predicted = torch.max(output1.data, 1)\n",
        "    _, predicted2 = torch.max(output2.data, 1)\n",
        "    \n",
        "    total += labels.size(0)\n",
        "    \n",
        "    correct1 += (predicted == labels).sum().item()\n",
        "    correct2 += (predicted2 == labels).sum().item()\n",
        "    \n",
        "print(\"Accuracy of the network using my batch norm: %2.3f %%\" % (100 * correct1 / total))\n",
        "print(\"Accuracy of the network using pytorch's built-in batch norm: %2.3f %%\" % (100 * correct2 / total))\n",
        "print(\"Diff: %.3f\" % ((100 * correct1 / total) - (100 * correct2 / total)))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of the network using my batch norm: 59.060 %\n",
            "Accuracy of the network using pytorch's built-in batch norm: 59.500 %\n",
            "Diff: -0.440\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}